Tacotron 논문 ▶
> [Tacotron: Towards End-to-End Speech Synthesis](https://arxiv.org/abs/1703.10135)

Tacotron2 논문 ▶
> [Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions](https://arxiv.org/abs/1712.05884)

## Cotatron 과 Tacotron
cotatron 은 speaker 의 특징에 구애받지 않고 any-to-many 방식의 음성변조를 할 수 있는데, 여기에서 가장 핵심이 되는 것이 tts 모델의 활용이다. 

이 tts 모델로 사용된 것이 Tacotron 2 인데, 이제부터  Tacotron 의 구조에 대해 알아보자.

✔ `seq2seq 과 bahdanau attention 에 대해 숙지한 다음 읽는 것을 추천한다`
## what is Tacotron?
![tacotron](https://gitlab.com/hanish3464/voice-conversion/uploads/b08bd2a834c36bc566b313f17731092b/tacotron.png)
```
tacotron은 텍스트를 입력하면 음성을 출력하는 tts(text-to-speech) 모델 중 하나이다.
(2017년 발표)
Tacotron 2 는 타코드론1과 WaveNet 같은 이전 논문에서 얻은 아이디어를 결합하고 개선사항을 
추가한 버전이다.
(2018 년 발표)

* WaveNet : 매우 강력한 오디오 생성 모델, 성능이 매우 좋아 Tacotron 의 Vocoder 로도
사용됨. 그러나 너무 느리고 end-to-end 가 아니라는 단점이 있음.
```
* 특징
```
* 기존의 모델들이 비해 End-To-End TTS systme 이기 떄문에 쉽게 사용할 수 있다.
(간단하게 <text, audio> 페어를 이용하여 학습 할 수 있다는 것.)

* 그러나 결과물의 길이가 Input 보다 훨씬 길다는 점에서 Error 가 발생하거나 결과물에
일관성이 없을 수 있으므로 신호 레벨에서 큰 변화에 대처할 수 있는 유연성을 학습시킬 필요가
있다.
```

## Architecture
`우선 tacotron 1 의 구조를 기준으로 설명한다.`
![tacotron3](https://gitlab.com/hanish3464/voice-conversion/uploads/4edafa827c8bd3bfaa04770721d778ab/tacotron3.png)
```
tacotron 은 기본적으로 encoder-edcoder 구조를 가진 seq2seq 모델이다.
기본적인 형태의 seq2seq 가 인코더와 디코더를 각각 RNN 으로 구현하는 것과 달리
타코트론에서 디코더는 RNN 의 형태를 가지고 있지만 인코더는 일반적인 신경망의 구조를
가지고 있다.

또한 모델에 attention mechnism 을 이용하며, 그 중 Bahdanau 를 사용하고 있다.
이 attention 이 cotatron voice conversion decoder 의 입력값을 생성하는데 핵심적인
개념이므로 주의깊게 볼 필요가 있다.   

아래의 프로세스 설명 파트에서 처리흐름을 좀 더 구체적으로 설명한다.
```

## Process
* 입출력
```
input : 문장 텍스트를 단어 단위로 embeddings 한 것

output1 (mel-spectrogram) : 디코더 파트에서 각 time step 마다 나오는 결과값.
                            mel-spectrogram 형태를 가지며 시간순서대로 출력됨.
							
output2 (audio) : 디코더 파트에서 출력된 mel-spectrogram 을 기반으로 audio 를 생성함.
```
![tacotron2](https://gitlab.com/hanish3464/voice-conversion/uploads/c62fc6415f52aea1c90256574866241b/tacotron2.png)

### 1. Encoder
```
Fully Connected Layer 과 1D Convolutional Layer 가 섞여 있는 network.
크게 Pre network 와 CBHG network 로 나뉜다.  

아웃풋은 입력배열과 동일한 갯수의 원소를 가지는 것으로 추정됨. 
(그래야 attention value 계산을 할 수 있다.)
```
![tacotron4](https://gitlab.com/hanish3464/voice-conversion/uploads/1b2bae02cbd5e148c7c9bf530f4eb883/tacotron4.png)

▶ Pre-net

_input(text embedding)_ → **FC(Dense) — ReLU — Dropout — FC — ReLU — Dropout**

▶ CBHG network

**Conv1D bank — Max Pooling — Conv1D projection — Conv1D Layer**

_Conv1D Layer + First Input_ → **Highway Net(4 layers of FC-ReLU) — Bidirectional RNN**

`CBHG 이미지`

![CBHG](https://gitlab.com/hanish3464/voice-conversion/uploads/a9cff66780e3570adfcfa4f49ab4ebad/CBHG.png)

### 2. Decoder
* **content-based tanh attention decoder**
```
attention mechanism 을 사용하는 seq2seq decoder 구조를 기본으로 함.

다음은 각 time step 마다의 처리과정이다.
```
1. **입력값** : 이전 타임 스탭의 출력값 (mel-spectrogram 형식)
2. pre-net : 입력값 처리 (encoder 의 pre-net 과 구조 동일)
3. Attention RNN : GRU Cell
4. attention module : **encoder output** 과 **Attention RNN 셀의 hidden state** 를 이용하여 Bahdanau 연산으로 Attention value계산.
5. Decoder RNN : Residual GRU Cell 2층 

![tacotron5](https://gitlab.com/hanish3464/voice-conversion/uploads/95fc62029c1170bec42d88b46b110fb8/tacotron5.PNG)


▶ tacotron decoder 는 이전 문서로 작성했던 **Bahdanau mechanism 의 attention value
 수식**을 사용하지만 네트워크의 구조는 다르므로 주의 필요

▶ 4번 과정에 attention module 계산을 한다고 나와있는데, 그림 상으로는 rnn 타입 스탭 중 가장 왼쪽의 attention rnn 셀만 어텐션 모듈과 연결되어 있는 것처럼 표현되어 있다.

▶ 하지만 실제로는 아래 그림과 같이 **모든 타입스탭마다** attention 모듈이 hidden state 값을 받아 attention value(context vector) 를 계산하는 것으로 이해해야 한다.

▶ Decoder RNN 의 입력값으로는 계산된 **CONTEXT VECTOR 와 attention rnn 셀의 hidden state 를 concatenate** 한 값이 들어간다.

(기본적으로 rnn 구조이기 때문에 이전 타임스탭에서 hidden state를 전달받음 이 값과 입력값을 이용하여 계산.)

![tacotron5-2](https://gitlab.com/hanish3464/voice-conversion/uploads/f8e166a051839f04e1c6f971d879e13d/tacotron5-2.png)

* Seq2seq target with r=3
```
decoder는 의 한 타임스탭마다 r개의 frame를 예측.
n(hz) 로 오디오를 샘플링했다고 가정하면, (1/n) * r 초 길이의 mel-spectrogram을 
예측한다는 것.

r 개의 frame 중 가장 마지막 것이 다음 타입스탭의 입력으로 들어간다.
```
### 3. Post-net & vocoder
```
이전 레이어에서 output 으로 나온 mel-spectrogram 를 이용하여 .wav 형식의 audio 파일을 
생성한다. 

이렇게 대사 text 를 인풋으로 넣으면 아웃풋으로 audio 가 나오는 형식을 갖추고 있기 때문에
End-to-End 로 간편하게 학습시킬 수 있음.
```
![tacotron6](https://gitlab.com/hanish3464/voice-conversion/uploads/037d99e83537e7e47b64e4bb9a3388d9/tacotron6.PNG)

▶ CBHG 레이어 : 이부분을 post-net 이라 함. encoder에서 사용되는 CBHG 레이어와 몇가지 파라미터를 제외하고는 거의 동일.

▶ Griffin-Lim reconstruction : spectrogram 을 다시 audio 로 만들어 주는 알고리즘.

----------------
### Learning

```
학습은 decoder 의 결과로 나오는 mel-spectrogram 과 이것이 CBHG 를 거친 
linear-scale spectrogram 두가지를 기준으로 한다.

따라서 loss 는 다음과 같다.
```

▶ **mel_loss = tf.reduce_mean(tf.abs(mel_outs - mel_targets))**

▶ **linear_loss = tf.reduce_mean(tf.abs(linear_outs - linear_targets))**

▶ **loss=mel_loss + linear_loss**

* Adam Optimizer 사용 : Learning rate 를 순차적으로 줄여나감.


# Tacotron VS Tacotron 2

`아래는 tacotron 2 의 아키텍처 그림이다.`

![taco2tron_3_](https://gitlab.com/hanish3464/voice-conversion/uploads/573732259fbbbb5ea51bffcaf50444dc/taco2tron_3_.PNG)

이렇게 보면 위에서 설명한 tacotron 과 완전히 다른 구조를 가진 것처럼 보이지만, 
**사실은 그래프를 아래와 같은 형태로 재배열**할 수 있다.

![taco2tron_4_](https://gitlab.com/hanish3464/voice-conversion/uploads/ef4ab8ce74c7a0eb842932d7f11e9671/taco2tron_4_.png)
```
그림에서 볼 수 있듯이 tacotron 2 는 기본적으로 tacotron 1 과 유사한 구조를 가진다. 
decoder 부분또한 그림이 축약되어 바로 알아보지 못할 수 있지만, 기본 tacotron 처럼 
rnn 네트워크를 기본 틀로 하고 입력 부분에 pre-net 이, 출력 부분에 post-net 이 위치하고
있다. 
```
`decoder 부분에서 좀더 세부적으로 어떤 차이가 있는지 보자. 다음은 decoder 부분을 확대하여 기존 tacotron 모델과 최대한 비슷한 형태로 묘사한 그래프의 이미지이다.`
![taco2tron](https://gitlab.com/hanish3464/voice-conversion/uploads/4a7cf9e12ba1c609b270fae5a5c1e798/taco2tron.png)
```
여기서 tacotron 과 tacotron2 의 차이점을 정리하면 다음과 같다.
```

1. CONTEXT VALUE 가 Decoder RNN 뿐만 아니라 **다음 time step** 에서 post-net 을 거친 spectrogram과 concat 되어 입력값으로 사용된다.
2. 각 RNN 셀에서 GRU 가 아닌 **LSTM** 이 사용된다. 
3. POST-NET 에 CBHG 가 아닌 **5층의 CNN 레이어**가 사용된다.
4. spectrogram 을 wav 로 변환하는데 **WaveNet vocoder 를 사용**한다.
5. vocoder 의 입력이 linear spectrogram 이 아닌 **mel spectrogram** 이다.

`
위의 차이점들은 전체적인 구조를 바꿀정도는 아니므로 기본 tacotron 을 기반으로 이해하면 된다.
좀 더 깊은 이해가 필요할 경우 tacotron 의 구현체 코드를 읽어보는 것을 추천한다. 
`
🤔

-------------
Reference : 

[Tacotron 세미나 - TmaxData NLP - youtube](https://www.youtube.com/watch?v=02odSrfgasI)

[# [Speech Synthesis] Tacotron 논문 정리](https://hcnoh.github.io/2018-12-11-tacotron)

[# 고등학생이 해석한 Tacotron2](https://medium.com/wasd/%EA%B3%A0%EB%93%B1%ED%95%99%EC%83%9D-%EC%8B%9C%EC%A0%90%EC%9C%BC%EB%A1%9C-%EB%85%BC%EB%AC%B8%EC%9D%84-%EC%9D%BD%EC%96%B4%EB%B3%B4%EC%9E%90-e6953caf4bf8)

[### 컴공학부생이 읽어보는 논문감상 - Tacotron : Toward End-To-End Speech Synthesis (1)](https://snowapril7758.tistory.com/4?category=760801)
