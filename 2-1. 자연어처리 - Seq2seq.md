> 본 문서는 wikidocs **"딥러닝을 이용한 자연어 처리 입문"** 문서의 다음 파트를 참고하여 작성했습니다.
> * [시퀀스-투-시퀀스(Sequence-to-Sequence, seq2seq)](https://wikidocs.net/24996)
> * [어텐션 메커니즘 (Attention Mechanism)](https://wikidocs.net/22893)

# Sequence-to-Sequence
### Cotatron 에서의 TTS model
이번 프로젝트에서 사용하는 Cotatron 모델은 speech encoder 를 사용하여 speaker 의 특징에 구애 받지 않고 언어(linguistic) 적인 특성을 나타낼 수 있는 방법을 제안한다.

![cotatron_h](https://gitlab.com/hanish3464/voice-conversion/uploads/d68eae9bb17c7a9f8388cec9f45621cc/cotatron_h.png)

Cotatron 은 multispeaker TTS(Text-to-Speech) 구조 및 데이터 셋을 기본으로 하여 학습된다. 
때문에 이 과정을 제대로 이해하기 위해서는 tts 모델의 기본이 되는 seq2seq 에 대해 정리할 필요가 있다고 생각되어 본 문서를 작성하였다.

`* Cotatron 에서는 tts 모델 중 Tacotron 2 를 사용한다.`

------------------------
 ### RNN 구조
 ``` 
 seq2seq 아키텍처는 RNN 구조을 기본으로 하여 구성되어 있다. 
 아래는 그림은 베이직한 RNN의 아키텍처이다. 
 ```
 `* 본 문서는 독자가 기존에 RNN에 대해 이해하고 있는 것을 전제로 하므로 추가적인 설명은 하지 않음.`
![RNN](https://gitlab.com/hanish3464/voice-conversion/uploads/4c87d526bacf2ea51e1f4cf43fe778b4/rnn.PNG)

## Seq2Seq 이란
`* Seq2Seq = Sequence-to-Sequence`
`입력 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 다양한 분야에서 사용`
* Seq2Seq 활용
```
1. 챗봇
2. 기계 번역
3. 내용 요약(Text Summarization)
4. STT(Speech to Text)
5. TTS(Text to Speech) >> 현제 우리 프로젝트에서 사용하는 개념
```
## Seq2Seq Architecture
> 다음과 같이 문장을 번역하는 상황을 예시로 한다

![](https://wikidocs.net/images/page/24996/%EC%8B%9C%ED%80%80%EC%8A%A4%ED%88%AC%EC%8B%9C%ED%80%80%EC%8A%A4.PNG)

* seq2seq 구성
```
크게 인코더와 디코더 두개로 구성. 
컨텍스트 벡터(context vector): 인코더가 입력 문장의 모든 단어들을 순차적으로 입력받아 
마지막에 압축된 정보로 만든 벡터. 
디코더는 컨텍스트 벡터를 받아 번역된 단어를 한 개씩 순차적으로 출력.
```
![](https://wikidocs.net/images/page/24996/seq2seq%EB%AA%A8%EB%8D%B811.PNG)![](https://wikidocs.net/images/page/24996/%EC%BB%A8%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%B2%A1%ED%84%B0.PNG)

`* 실제 현업 사용되는 모델의 context vecter는 보통 수백 이상의 차원을 가진다`

* 내부 구조
```
인코더와 디코더는 RNN 구조를 가지고 있음. 
(아래 이미지의 LSTM 은 RNN 의 종류 중 하나)
1. Encoder 가 입력 문장을 받고 RNN 연산 실행
2. 인코더 셀의 은닉 상태를 RNN 셀로 넘김 (이것이 Context vector)
3. 디코더 셀이 context vector 를 첫번째 은닉상태로 사용하여 출력문장 생성 

<sos> : 디코더의 초기 입력으로 문장의 시작을 의미하는 의미하는 심볼.  

training : 모델에서 나온 결과와 실제 번역 문장의 차이를 기준으로 학습.
```
![](https://wikidocs.net/images/page/24996/%EC%9D%B8%EC%BD%94%EB%8D%94%EB%94%94%EC%BD%94%EB%8D%94%EB%AA%A8%EB%8D%B8.PNG)

* 데이터 전처리
```
워드 임베딩 : 텍스트를 벡터로 변환하는것. 
모든 단어들은 사용하기 전, 워드 임베딩을 통해서 임베딩 벡터로 변환함.
```
![](https://wikidocs.net/images/page/24996/%EC%9E%84%EB%B2%A0%EB%94%A9%EB%B2%A1%ED%84%B0.PNG)

* 작동 원리
```
RNN 의 개념상 인코더에서 넘기는 마지막 은닉 상태는 과거 시점 RNN 셀들의 모든 은닉 상태의
값들의 영향을 누적한 결과라고 할 수 있음. 그렇기 때문에 컨텍스트 벡터가 입력 문장의 모든 
단어 토큰 정보를 요약해서 담고있다고 할 수 있는것.

디코더는 하나의 단어를 예측할 때마다 
(출력 벡터에 소프트맥스를 적용시켜 확률값으로 변환한 결과) vs (워드 임베딩 벡터들)
을 비교하여 가장 유사한 것으로 단어를 결정. 
```
![](https://wikidocs.net/images/page/24996/decodernextwordprediction.PNG)

## 더 복잡한 seq2seq
```
1. (기본) 컨텍스트 벡터를 디코더의 초기 은닉 상태로만 사용
2. 컨텍스트 벡터를 디코더의 모든 타임스텝에 입력으로 사용
3. 어텐션 메커니즘을 사용해 더욱 문맥을 반영할 수 있는 컨텍스트 벡터를 생성하여 
   매 시점마다 하나의 입력으로 사용 
(이 부분에 대해서는 아래의 내용으로 자세히 작성)
``` 

# Attention Mechanism
* 기존 seq2seq 모델의 한계
```
위의 기본 모델은 인코터의 시퀀스를 하나의 고정된 크기의 벡터로 압축하고(context vector), 
디코더는 그것을 기반으로 출력 시퀀스를 생성했다. 
그러나 이런 구조에서는 크게 두가지 문제가 존재한다.

1. 하나의 고정된 크기의 벡터에 모든 정보를 압축하면서 정보 손실 발생
2. RNN 의 고질적인 문제인 기울기 소실(Vanishing Gradient) 문제 존재

> 즉, 입력 시퀀스가 길어지면 출력 시퀀스의 정확도가 떨어지는 것을 보정하기 위한 기법이 필요
> 그것이 어텐션(Attention) 기법
```

* Attension 의 아이디어
```
디코더의 모든 타임 스탭에서 인코더에 입력된 전체 문장을 참고하는 것.
단, 입력 문장 전체를 동일한 비율로 참고하지 않고, 해당 시점에서 입력 단어 중 어떤 부분에 
더 집중(attention) 해서 볼지 선택함.
```

## Attention Mechanism Architecture
* 어텐션 함수
**Attention(Q, K, V) = Attention Value**
```
어텐션 함수의 작동

1. 주어진 '쿼리(Query)'에 대해서 모든 '키(Key)'와의 유사도를 각각 계산.
2. 유사도를 키와 맵핑되어있는 각각의 '값(Value)'에 반영
3. 유사도가 반영된 '값(Value)' 들을 적용하여 최종적으로 어텐션 값(Attention Value) 출력
```
![](https://wikidocs.net/images/page/22893/%EC%BF%BC%EB%A6%AC.PNG)
* seq2seq 에서의 Attension
```
Q = Query : t 시점의 디코더 셀에서의 은닉 상태  
K = Keys : 모든 시점의 인코더 셀의 은닉 상태들  
V = Values : 모든 시점의 인코더 셀의 은닉 상태들
```

## 닷-프로덕트 어텐션(Dot-Product Attention)
> 어텐션의 다양한 종류 중 가장 수식적으로 단순한 닷-프로턱트 어텐션을 예시로 설명한다.

`seq2seq 에서 사용되는 어텐션 종류들 끼리는 중간 수식의 차이가 있을 뿐, 메커니즘 자체는 거의 유사하다.`

* 내부 구조
```
아래의 그림에서 디코더가 세번째 단어를 예측하는 중이라고 가정하자.
여기서 softmax 를 거친 결과값은 인코더에 입력된 각각의 단어가 출력 단어를 예측하는데 
어느정도 도움이 되는지(= 어느정도 attention 을 가지는지 = 어느정도 유사도를 가지는지) 
의 정도를 수치화한 값이다.

이것이 계산되면 모아서 하나의 정보로 담아 다시 디코더로 전송된다.
이렇게 디코더의 각 타임스탭마다 인코더의 정보를 활용하여 더 정확한 예측을 할 수 있는 것이다.  
```
`* softmax 위의 빨간 직사각형의 크기는 소프트맥스 함수의 결과값 크기를 표현`
![](https://wikidocs.net/images/page/22893/dotproductattention1_final.PNG)
```
❕ 위의 내용은 어텐션 메커니즘에 대한 감을 잡기위한 전체적인 그림이기 때문에 이것만으로는 
제대로 이해하기 힘들 것이다. 이제부터 메커니즘의 프로세스에 대해 상세히 알아보자.
```

## Attention Mechanism Process
`디코더의 한 타임스탭에서 결과값을 구하기까지`

* setting
	* 디코더의 현재 시점 t 에서의 디코더의 은닉 상태  <img src="https://latex.codecogs.com/svg.latex?h_x "/> 를 기준으로 설명함.
	* 인코더의 시점(time step) 을 각각 1, 2, ... N 라고 할때, 인코더의 은닉 상태(hidden state) 는 각각 <img src="https://latex.codecogs.com/svg.latex?h_1 "/>, <img src="https://latex.codecogs.com/svg.latex?h_2 "/>, ... <img src="https://latex.codecogs.com/svg.latex?h_N "/>
	* 인코더의 은닉 상태와 디코더의 은닉 상태는 차원이 같다고 가정 (이 예시에서는 각각 4차원)

```
시점 t에서 디코더 셀은 두개의 입력값을 필요로 한다.
* 이전 시점 t-1 의 은닉 상태
* 이전 시점 t-1 의 출력 단어

어텐션 메커니즘에서는 여기에 또 다른 값이 추가로 필요한데, 이것이
"어텐션 값(Attention Value)" 이다.

아래에 나오는 계산들은 모두 어텐션 값을 구해 결과에 반영하기 위한 과정이다.  
```

-------------
1. **어텐션 스코어**(Attention Score) : <img src="https://latex.codecogs.com/svg.latex?e^t "/>
	```
	현재 디코더의 시점 t에서 단어를 예측하기 위해
	(인코더의 모든 은닉 상태 각각) vs (디코더의 현 시점 은닉 상태)
	의 유사도를 판단한 스코어 값.
	```
	![](https://wikidocs.net/images/page/22893/dotproductattention2_final.PNG)
	* <img src="https://latex.codecogs.com/svg.latex?s_t "/> 를 전치(transpose) 하고 각 은닉 상태와 내적(dot product)를 수행
	* 스칼라 형태의 어텐션 스코어 출력

	![](https://wikidocs.net/images/page/22893/i%EB%B2%88%EC%A7%B8%EC%96%B4%ED%85%90%EC%85%98%EC%8A%A4%EC%BD%94%EC%96%B4_final.PNG)

  ▶ 어탠션 스코어 식 표현

![](https://latex.codecogs.com/gif.latex?score%28s_t%2C%20h_i%29%3Ds_t%5ET%20h_i)

  ▶ 모든 은닉 상태에 대한 어텐션 스코어의 모음값을 <img src="https://latex.codecogs.com/svg.latex?e^t "/> 라고 정의할 때,

![](https://latex.codecogs.com/gif.latex?e%5Et%3D%5Bs_t%5ET%20h_1%2C%20...%2C%20s_t%5ET%20h_N%5D)

2. **어텐션 분포**(Attention Distribution) : <img src="https://latex.codecogs.com/svg.latex?a^t "/>
	```
	어텐션 스코에 모음벡터인 e_t 에 소프트맥스 함수를 적용하면, 합이 1이 되는 확률 분포를
	얻을 수 있음. 
	이것이 어텐션 분포(Attention Distribution) 이고,
	각각의 값은 어텐션 가중치(Attention Weight) 이다.
	
	이 가중치가 (하나의 디코더 셀) vs (특정 입력 단어) 가 가지는 유사도가 된다.
	즉, 값이 클수록 해당 입력 단어가 출력 단어에 영향을 끼치는 정도가 큰 것.
	```
	![](https://wikidocs.net/images/page/22893/dotproductattention3_final.PNG)

	▶ 어텐션 분포 식 표현

![](https://latex.codecogs.com/gif.latex?a%5Et%3D%20softmax%28e%5Et%29)

3. **어텐션 값**(Attention Value) : <img src="https://latex.codecogs.com/svg.latex?a^t "/>
	```
	각 인코더의 은닉 상태와 어텐션 가중치값들을 곱하고, 최종적으로 더한다.
	> 즉, 가중합(Weighted Sum) 을 하는 것.
	```
	![](https://wikidocs.net/images/page/22893/dotproductattention4_final.PNG)

	▶ 어텐션 값 식 표현
  
  ![](https://latex.codecogs.com/gif.latex?a_t%3D%20%5Csum_%7Bi%3D1%7D%5EN%20a_i%5Et%20h_i)
	
`* 어텐션 분포와 어텐션 값 기호를 햇깔리지 말자`

4. **어텐션 값(<img src="https://latex.codecogs.com/svg.latex?a_t "/>) & 디코더 은닉 상태(<img src="https://latex.codecogs.com/svg.latex?s_t "/>)  연결** : <img src="https://latex.codecogs.com/svg.latex?v_t "/>
	```
	"어텐션 값"과 "타깃 디코더 셀의 은닉 상태" 를 결합(concatenate)한 벡터를 생성한다.
	
	이 v_t 를 hat(y) 예측 연산의 입력으로 사용함으로써 인코더로부터 얻은 정보를 활용하는
	것. 이것이 어텐션 매커니즘의 핵심이다.
	```
	![](https://wikidocs.net/images/page/22893/dotproductattention5_final_final.PNG)

	▶ 결합 벡터 표현 식
  
![](https://latex.codecogs.com/gif.latex?v_t%3D%20%5Ba_t%3B%20s_t%5D)

5. **디코더 출력층 입력값** : <img src="https://latex.codecogs.com/svg.latex?\tilde{s_t} "/>
	```
	v_t 를 출력층으로 보내기 전, 하이퍼볼릭탄젠트 함수를 지나도록 하여 기존 은닉상태(s_t) 
	와 같은 차원이 나오도록 변환.
	```
	![](https://wikidocs.net/images/page/22893/st.PNG)

	▶ 출력층 입력값 계산 식

![](https://latex.codecogs.com/gif.latex?%5Ctilde%7Bs_t%7D%20%3D%20tanh%28W_c%20v_t%20&plus;%20b_c%29)

`그림에서는 편향이 생략됨`

6. **결과값**

	▶ <img src="https://latex.codecogs.com/svg.latex?\tilde{s_t} "/> 를 출력층의 입력으로 사용하여 예측 벡터 계산

![](https://latex.codecogs.com/gif.latex?%5Chat%7By_t%7D%20%3D%20Softmax%28W_y%20%5Ctilde%7Bs_t%7D%20&plus;%20b_y%29)

## 다양한 종류의 어텐션(Attention)

`위에 적었듯, dot-product 연산과 다른 어텐션들과의 차이는 Attention score 계산 수식의 차이임.`
![attention_score](https://gitlab.com/hanish3464/voice-conversion/uploads/8d9034ce7bc2c6a781c7d1dfd062d6f1/attention_score.PNG)
