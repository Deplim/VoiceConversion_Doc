> [GITHUB : mindslab-ai/cotatron](https://github.com/mindslab-ai/cotatron) 의 코드를 기반으로 합니다.

✅ `cotatron_trainer.py 학습`

✅ `영어 데이터로 학습시킬 경우 위의 코드를 변경하지 않아도 됩니다.`

## 🧿 TARGET
학습 이전 세팅해야할 것은 다음과 같이 크게 세가지입니다.
1. metadata (.txt)
2. audio data (.data)
3. config (.yaml)

## 💾 DATA INSTALL
* **What is VCTK? :**

	`speech data`, `english`, `108 speaker (400 sentences each)`

|
* **Download :**

	[http://www.udialogue.org/download/VCTK-Corpus.tar.gz](http://www.udialogue.org/download/VCTK-Corpus.tar.gz)

	`다운로드는 구글에 VCTK dataset 이라고만 검색해도 나오므로 위의 주소로 인스톨이 안될 경우 따로 찾으면 됨.`

|
* **Folder**
	```
	txt # trainscription
	VCTK-Corpus # wav audio files classified by speakerId
	COPYING 
	README
	speaker-info.txt
	```
## 📣 Audio data
* **VCTK-Corpus 의 내용물을 원하는 폴더로 옮겨놓을 것**

	`여러 데이터를 대상으로 테스트해 볼 것이라면 한 폴더에 데이터들을 관리하는 것을 추천`

|
* **Data resampling**

	`논문상에서도 오디오 데이터를 22.05kHz 로 리셈플링한다고 나와 있음. cotatron 플젝 datasets 폴더에 있을 resample_delete.sh 을 본인의 데이터가 있는 위치로 옮긴 후 bash 실행`
	```
	bash resample_delete.sh
	```
	> 실행하면 원래의 파일은 삭제되므로 주의.

## 📔 Metadata
* **Metadata format**
	```
	audio_file_path|trainscription|speakerId
	```
	**ex)**
	```
	VCTK-Corpus/p225/p225_002-22k.wav|ask her to bring these things with her from the store|p225
	```

	> ❕ vctk set 은 따로 메타데이터 만들 필요 없음. 우리가 사용하는 플젝에서 사용하는 예시가 vctk 이기 때문에 datasets 의 metadata 에 이미 포함되어 있습니다.`

## 🧾 Config
### global/vctk.yaml : `yaml 파일 이름은 자유. 학습 실행시킬때 지정만 잘 해주면 됨.`

* **lang** 

	`디폴트는 cmu, eng로 바꿀것.`
	> cmu 로 설정하면 대사의 word 하나하나를 영어 발음기호로 변환하도록 되어있음. vctk 에서는 굳이 그럴 필요는 없다고 판단.

|
* **text_cleaners** :

	`['english_cleaners']`
	> 언어에 따라 word 전처리 역할을 할 cleaners 지정 필요. 

|
* **speakers** :

	`List of speakers ID`

	**ex)**
	```
	['p225','p226', 'p227', 'p228', 'p229', ...
	```
	> 108명의 id 를 전부 적어주어야 함. 직접 적어도 되지만, 이 경우에는 speaker 수가 많으니 다른 방법을 쓰는 것을 추천. 작성자의 경우 vctk speaker-info.txt 에서 id 만 가져와 위와 같이 정렬하도록 파이썬 코드 작성해 출력함.

|
* **train_dir, val_dir** :

	`데이터 폴더 경로`
	> ❕ train_dir+(matadata file path) 로 정확히 오디오 파일이 찾아져야 하므로 metadata 파일과 자신의 데이터 저장 경로를 정확히 보고 적을것.

|
* **train_meta, val_meta** :

	`datasets/metadata/vctk_22k_train.txt` 
	`datasets/metadata/vctk_22k_val.txt`
	> datasets 폴더의 metadata 를 그대로 쓴다고 가정함.

-----
```
global yaml 파일에서 data 파트 아래쪽으로는 거의 건들일 없을거라고 보고 참고만 하면 됨. 

조정해야할 하이퍼 파라미터들은 거의 다 cota yaml 에 있음.
```	
### cota/vctk.yaml
* 기본으로 있는 `cota/default.yaml` 가 이미 vctk, LibriTTS 기준으로 맞춰진 파라미터를 가지고 있으므로 그대로 사용해도 무방함.

## 📟 RUN
`cotatron_trainer.py 의 학습준비가 되었고 학습된 모델을 생성했다면 synthesizer_trainer.py 부분도 학습시킬 수 있다. synthesizer 문서는 따로 작성하지 않음.`
```
python cotatron_trainer.py -c {global config path} {cota config path} -g {gpu to use} -n {run_name}
```
ex)
```
python cotatron_trainer.py -c config/global/vctk.yaml config/cota/vctk.yaml -g 0,2 -n vctk_test
```
> ❗ 프로젝트 전체를 의미하는 cotatron 과 모델 중 linguistic feature 를 추출하기 위한 speech encoder 를 의미하는 cotatron을 잘 구분해서 이해해야 함.
